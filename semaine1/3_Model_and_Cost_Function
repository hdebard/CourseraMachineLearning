# # # # # # # # # # #
# 	  	  	  	  	#
#	   Model		#
#		and			#
#	Cost Function	#
#					#
# # # # # # # # # # #

# Model Representation #

  	Subject: Supervised Learning - Linear regression

	Notation:

		_ {} = superscript, [] = subscript

		_ /~/~ Dataset Correctly Parsed = training set

		_ m = number of training example / size of training set

		_ x = input variable, feature

		_ y = output variable, target variable

		_ (x,y) - one training exemple

		_ (x{i},{i}) - i{th} training example

		  		Example: _ x{1} input variable of training example 1
						 _ y{2} output variable of training example 2

		_ h[theta](x) = theta[0] + theta[1] * x = h(x)
		  		Linear reggression with one variable

	Model:

		- Training Set -> Learning Algorithm -> Hypothesis

		                                       x  ->|-> y (hypothesis map x to y)

# Cost Function #

  	Hypothesis:  _ h[theta](x) = theta[0] + theta[1] * x

				_ theta[0] and theta[1] = Parameters

	Goal: _ minimize the square difference between hypothesis and output
											 		|-> h(x).      |-> y.
		    = minimize [theta[0], theta[1]]      J(theta[0], theta[1])
			  		   			  			 	 		|-> COST FUNCTION

	Formula:

			- Hypothesis:  _ h[theta](x) = theta[0] + theta[1] * x

			- square difference between hypothesis and REAL output:
					_ (m ∑ i) (h[theta](x{i}) - y{i}){2}

		  	- Cost function (Squared error function):

							  										|-> y = true output, not out put of h(x)	
		  			_ (1/(2 * m) * ?) (m ∑ i) (h[theta](x{i}) - y{i}){2}
		  		 	  		|-> learning rate     |-> = theta[0] + theta[1]*x{i}

			  	   _ J(theta[0], theta[1]) = (1/(2 * m) * ?) (m ∑ i) (h[theta](x{i}) - y{i}){2}

	Explaination:
			_ Squared error function is the most used cost function for linear reggression problem


# Cost Function - Intuition I #

  	 -  Rappel : _ Hypothesis: h[theta](x) = theta[0] + theta[1] * x
	   		    _ Parameters: theta[0] and theta[1]
				_ Cost Function: J(theta[0], theta[1]) = (1/(2 * m) * ?) (m ∑ i) (h[theta](x{i}) - y{i}){2}
			 	_ Goal: minimize [theta[0], theta[1]]     J(theta[0], theta[1])


	 - Example:

	 		_ Hypothesis simplified: h[theta](x) = theta[1]x
			  			 	|-> theta[0] = 0

	 - YOLO:

			_ h[theta](x) = hypothesis = for fixed theta, this is a function of x

			_ J(theta's) = Cost Function = function of theta = The good theta will give us the minimum 

	 - Example (theorical dataset):
					_ Training set: y{1} = 1, y{2} = 2, y{3} = 3

					_ Hypothesis: _ h(x) = theta[1] * x

					_ if theta[1] = 1: _  ---->  x = y
					        		   _  ---->  J(1) = 0
													 
					_ if J(theta) = 0, we find the minimum, h(x) fit the data perfectly


# Cost Function Function - Intuiton II #

  	 -  Rappel : _ Hypothesis: h[theta](x) = theta[0] + theta[1] * x
	   		    _ Parameters: theta[0] and theta[1]
				_ Cost Function: J(theta[0], theta[1]) = (1/(2 * m) * ?) (m ∑ i) (h[theta](x{i}) - y{i}){2}
			 	_ Goal: minimize [theta[0], theta[1]]     J(theta[0], theta[1])



	_ Dataset = house pricing

	_ Resume:

			_ Example of 3D plot of the J(x)
			_ Example of contour plotting J(x)n